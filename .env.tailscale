# Tailscale deployment configuration
# Use this when you want to expose the service via Tailscale

# Deployment Mode
DEPLOYMENT__MODE=remote
DEPLOYMENT__SERVICE_NAME=llm-translation-tailscale
DEPLOYMENT__NETWORK_INTERFACE=tailscale0
DEPLOYMENT__ENABLE_DISCOVERY=true
DEPLOYMENT__DISCOVERY_PORT=8889

# API Configuration - bind to all interfaces for Tailscale access
API__HOST=0.0.0.0
API__PORT=8000

# Ollama Configuration (local)
OLLAMA__OLLAMA_HOST=http://localhost:11434
OLLAMA__MODEL_NAME=llava:latest

# Redis Configuration (optional)
REDIS__REDIS_URL=redis://localhost:6379/0

# Authentication (can be enabled for external access)
AUTH__DISABLE_SIGNATURE_VALIDATION=false

# Security - allow Tailscale network access
API__CORS_ORIGINS=["*"]

# Environment
ENVIRONMENT=production
DEBUG=false

# Logging
LOGGING__LOG_LEVEL=INFO
